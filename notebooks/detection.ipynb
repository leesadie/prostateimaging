{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6d3a3c1",
   "metadata": {},
   "source": [
    "# Detection\n",
    "\n",
    "Predict if an image contains tumor(s) that are either clinically significant or not, where clinical significance refers to having a Gleason score greater than or equal to 7, volume greater than or equal to 0.5cc, and/or extraprostatic extension.\n",
    "\n",
    "Architecture: MONAI ResNet10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b20da2",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ba04be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from monai.networks.nets import resnet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36d30d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to sys path to allow for package-like imports\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6348df29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ec6f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device - MacOS\n",
    "device = torch.device( \"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5270107",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feac8cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "from scripts.load_data import MRIDataset\n",
    "\n",
    "dataset = MRIDataset(root_dir=\"../data/lesions\", labels_path=\"../data/lesions/PROSTATEx_Classes.csv\")\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67dfb919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patients: 199\n"
     ]
    }
   ],
   "source": [
    "# Patient-level index for train/test split\n",
    "patient_idxs = defaultdict(list)\n",
    "\n",
    "for idx, sample in enumerate(dataset.samples):\n",
    "    finding_id = sample[\"finding_id\"]\n",
    "    patient_id = finding_id.split(\"_Finding\")[0]\n",
    "    patient_idxs[patient_id].append(idx)\n",
    "\n",
    "patient_ids = list(patient_idxs.keys())\n",
    "print(f\"Total patients: {len(patient_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b86357b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 160\n",
      "Test samples: 40\n"
     ]
    }
   ],
   "source": [
    "train_patients, test_patients = train_test_split(\n",
    "    patient_ids,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_idxs = []\n",
    "test_idxs = []\n",
    "\n",
    "for pid in train_patients:\n",
    "    train_idxs.extend(patient_idxs[pid])\n",
    "\n",
    "for pid in test_patients:\n",
    "    test_idxs.extend(patient_idxs[pid])\n",
    "\n",
    "print(f\"Train samples: {len(train_idxs)}\")\n",
    "print(f\"Test samples: {len(test_idxs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b48a6c",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3bcfe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to iterate over torch Subset\n",
    "\n",
    "def iter_subset(subset):\n",
    "    for i in range(len(subset)):\n",
    "        yield subset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71894b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.preprocess import Compose, ResampleResize, Normalize, ToTensor, CropROI\n",
    "\n",
    "transform = Compose([\n",
    "    CropROI(margin=(12, 12, 4)),\n",
    "    ResampleResize(target_spacing=(0.5, 0.5, 3.0), target_shape=(96, 96, 16)),\n",
    "    Normalize(),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "train_set = Subset(MRIDataset(\n",
    "    root_dir=\"../data/lesions\", \n",
    "    labels_path=\"../data/lesions/PROSTATEx_Classes.csv\",\n",
    "    transform=transform\n",
    "), train_idxs)\n",
    "\n",
    "test_set = Subset(MRIDataset(\n",
    "    root_dir=\"../data/lesions\", \n",
    "    labels_path=\"../data/lesions/PROSTATEx_Classes.csv\",\n",
    "    transform=transform\n",
    "), test_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93fb220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loaders\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394f19ba",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "MONAI 3D ResNet\n",
    "- Input (1, 96, 96, 16)\n",
    "- Binary classification head (single neuron + sigmoid)\n",
    "\n",
    "Metrics\n",
    "- AUC, sensitivity, specificity\n",
    "\n",
    "Loss: BCEWithLogitsLoss and class weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75d7a916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv3d(1, 64, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3), bias=False)\n",
       "  (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool3d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): ResNetBlock(\n",
       "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): ReLU(inplace=True)\n",
       "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): ResNetBlock(\n",
       "      (conv1): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): ReLU(inplace=True)\n",
       "      (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2))\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): ResNetBlock(\n",
       "      (conv1): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): ReLU(inplace=True)\n",
       "      (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2))\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): ResNetBlock(\n",
       "      (conv1): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "      (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): ReLU(inplace=True)\n",
       "      (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2))\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = resnet.resnet10(\n",
    "    spatial_dims=3,\n",
    "    n_input_channels=1,\n",
    "    num_classes=1,\n",
    "    conv1_t_stride=1\n",
    ").to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e461061b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3w/wbtzr_qn1vsd0q77hb8f34tr0000gn/T/ipykernel_78011/3456841020.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pos_weight = torch.tensor(num_neg / num_pos).to(device)\n"
     ]
    }
   ],
   "source": [
    "# Weighted BCE loss\n",
    "num_pos = sum([s[\"cls_label\"] for s in iter_subset(train_set)])\n",
    "num_neg = len(train_set) - num_pos \n",
    "pos_weight = torch.tensor(num_neg / num_pos).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "# Adam optimizer\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9167c59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics helper\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    # Threshold at 0.5 for binary\n",
    "    y_bin = (y_pred >= 0.5).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_bin).ravel()\n",
    "\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"AUC\": auc,\n",
    "        \"Sensitivity\": sensitivity,\n",
    "        \"Specificity\": specificity\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "599047ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard logging\n",
    "writer = SummaryWriter(log_dir=\"runs/detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4d735c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train loss: 0.9578 | Val loss: 1.3999 | AUC: 0.5269 | Sensitivity: 1.0000 | Specificity: 0.0000\n",
      "Epoch 2/10 | Train loss: 0.4893 | Val loss: 1.3669 | AUC: 0.5627 | Sensitivity: 0.2353 | Specificity: 0.8261\n",
      "Epoch 3/10 | Train loss: 0.2056 | Val loss: 1.1189 | AUC: 0.6087 | Sensitivity: 0.4706 | Specificity: 0.7391\n",
      "Epoch 4/10 | Train loss: 0.1371 | Val loss: 1.8557 | AUC: 0.5754 | Sensitivity: 0.2353 | Specificity: 0.8261\n",
      "Epoch 5/10 | Train loss: 0.1422 | Val loss: 1.3310 | AUC: 0.5371 | Sensitivity: 0.4706 | Specificity: 0.5217\n",
      "Epoch 6/10 | Train loss: 0.1574 | Val loss: 1.9262 | AUC: 0.5575 | Sensitivity: 0.3529 | Specificity: 0.7826\n",
      "Epoch 7/10 | Train loss: 0.1343 | Val loss: 1.3506 | AUC: 0.6087 | Sensitivity: 0.8235 | Specificity: 0.2609\n",
      "Epoch 8/10 | Train loss: 0.1381 | Val loss: 2.3325 | AUC: 0.6368 | Sensitivity: 0.1765 | Specificity: 1.0000\n",
      "Epoch 9/10 | Train loss: 0.1553 | Val loss: 2.7709 | AUC: 0.5703 | Sensitivity: 0.1765 | Specificity: 0.8261\n",
      "Epoch 10/10 | Train loss: 0.1029 | Val loss: 1.1154 | AUC: 0.6394 | Sensitivity: 0.8235 | Specificity: 0.3913\n",
      "Best val AUC: 0.6394\n"
     ]
    }
   ],
   "source": [
    "from scripts.train_model import train_model\n",
    "\n",
    "trained_model = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,\n",
    "    optimizer=optim,\n",
    "    criterion=criterion,\n",
    "    metrics_fn=compute_metrics,\n",
    "    device=device,\n",
    "    epochs=10,\n",
    "    writer=writer,\n",
    "    task_name=\"detection\",\n",
    "    path=\"models/best_detection.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8b2d46",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "From `tensorboard`, we observe the following graphs:\n",
    "\n",
    "**Train loss**\n",
    "\n",
    "<img src=\"../assets/detection/detection_Loss_train.svg\" width=\"600\" height=\"200\">\n",
    "\n",
    "**Validation loss**\n",
    "\n",
    "<img src=\"../assets/detection/detection_Loss_val.svg\" width=\"600\" height=\"200\">\n",
    "\n",
    "**Validation AUC**\n",
    "\n",
    "<img src=\"../assets/detection/detection_AUC_val.svg\" width=\"600\" height=\"200\">\n",
    "\n",
    "**Validation sensitivity**\n",
    "\n",
    "<img src=\"../assets/detection/detection_Sensitivity_val.svg\" width=\"600\" height=\"200\">\n",
    "\n",
    "**Validation specificity**\n",
    "\n",
    "<img src=\"../assets/detection/detection_Specificity_val.svg\" width=\"600\" height=\"200\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prostatemr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
